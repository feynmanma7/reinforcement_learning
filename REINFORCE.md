# 1. Policy Gradient 

The goal of Q-learning is to learn Q function $Q(s, a)$, then select action with the largest Q value in current state, which obtains the policy **indirectly**. 

However, policy gradient **directly** learns the policy $\pi(a|s; \theta)$. 

## 1.1 Policy Gradient Theorem

+ Goal

Assume the policy is $\pi_{\theta}$，generate (sample) a trace $\tau$：
$(s_0, a_0, r_1, s_1, a_1, r_2, ...)$

The goal is to maximize the expectation of the accumulated return $\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$

Thus the goal function is defined as: 

> $J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$

+ Method to solve the problem: gradient ascent，compute the partial derivatives of $J$ with respect to $\theta$

> $\nabla_{\theta}J(\theta) = \nabla_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$

+ Use the defition of expectation

Define the probability of $\tau$ under policy $\pi_{\theta}$ as $P(\tau;\theta)$, the expection becomes,

> $\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$
> $= \int_{\tau} P(\tau;\theta) R(\tau) d \tau$

Compute the partial derivative, 

> $\nabla_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$
> $= \int_{\tau} \nabla_{\theta} P(\tau;\theta) R(\tau) d \tau$

+ Thanks to the constant equation
> $\nabla_{\theta} P(\theta) = P(\theta) \nabla_{\theta} log P(\theta)$

+ Use the **constant equation**

> $\nabla_{\theta}J(\theta) = \int_{\tau} P(\tau;\theta) \nabla_{\theta} log P(\tau;\theta) R(\tau) d \tau$

+ Use the definition of **expectation** again

> $\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\nabla_{\theta} log P(\tau;\theta) R(\tau)]$

+ Use the defintion of **probability**

If current time is $t$, only time after $t$ should be considered, 

> $P(\tau;\theta) = P(s_0)\pi(a_0|s_0)P(s_1|s_0, a_0)\pi(a_1|s_1)...$

> $=  P(s_0) \prod_{t=0}^T \pi_{\theta}(a_t|s_t) P(s_{t+1}|s_t, a_t) $

Use $log$ on both sides,

> $\log P(\tau;\theta) = \log P(s_0) + \sum_{t=0}^T \pi_{\theta}(a_t|s_t) + \sum_{t=0}^T P(s_{t+1}|s_t, a_t) $

Only $\pi_{\theta}$ has parameters $\theta$,

> $\log P(\tau;\theta) = \sum_{t=0}^T \pi_{\theta}(a_t|s_t)$

Thus,

> $\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[ R(\tau)  \sum_{t=0}^T \pi_{\theta}(a_t|s_t) ]$

+ $R(\tau)$ can be replaced by the return of time $t$, $G_t$

> $G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3}...$
$= \sum_{t=0}^{\infty} \gamma^{t} r_{t+1}$

Finally,

> $\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[ G_t  \sum_{t=0}^T \pi_{\theta}(a_t|s_t) ]$

+ Equivalent way.

$G_t$ is the MonteCarlo estimation of $Q(s_t, a_t)$, the 
original way using Q value is,

> $\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[ Q(s_t, a_t)  \sum_{t=0}^T \pi_{\theta}(a_t|s_t) ]$

+ Summay

> + Define the goal function of policy gradient, to maximize the expectation of return of trace generated by the policy.

> + Use the gradient ascent method, use defition of expectation and probability, with the help of a constant equation on partial derivative computing.

# 2. REINFORCE

Sample some episodes under current policy, compute current return, 

> $G_t = \sum_{t'=0}^T \gamma^{(t'-t)} r_t$ 

Update the parameters,

> $\theta \leftarrow \theta + \alpha \sum_t^T G_t \nabla_{\theta} log \pi_{\theta}(a_t | s_t)$


